{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensor09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONS/RAwu0WdnU8u3HkWbrz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhoujiuzhou9/tensorFlow/blob/V1/tensor09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0D1c9MdppvY",
        "outputId": "d152b1a5-ab9a-44c0-d33f-3fc950e183eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "print(tf.__version__)\n",
        "\n",
        "x = tf.constant([[2, 2], [3, 3]])\n",
        "y = tf.constant([[8, 16], [2, 3]])\n",
        "print(x @ y)\n",
        "\n",
        "a = tf.ones([4, 2, 3])\n",
        "b = tf.ones([4, 3, 5])\n",
        "c = a @ b\n",
        "print(c.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsz_jDce2B3i",
        "outputId": "f42b53af-4cd6-4623-90cc-0a0fe81cd408"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n",
            "tf.Tensor(\n",
            "[[20 38]\n",
            " [30 57]], shape=(2, 2), dtype=int32)\n",
            "(4, 2, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recap\n",
        "â–ª ð‘œð‘¢ð‘¡ = ð‘Ÿð‘’ð‘™ð‘¢{ð‘Ÿð‘’ð‘™ð‘¢ ð‘Ÿð‘’ð‘™ð‘¢ ð‘‹@ð‘Š1 + ð‘1 @ð‘Š2 + ð‘2 @ð‘Š3 + ð‘3}\n",
        "\n",
        "â–ª ð‘ð‘Ÿð‘’ð‘‘ = ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð‘œð‘¢ð‘¡)\n",
        "\n",
        "â–ª ð‘™ð‘œð‘ ð‘  = ð‘€ð‘†ð¸(ð‘œð‘¢ð‘¡, ð‘™ð‘Žð‘ð‘’ð‘™)\n",
        "\n",
        "â–ª minimize ð‘™ð‘œð‘ ð‘ \n",
        "\n",
        "â–ª [ð‘Š1\n",
        "\n",
        "â€²\n",
        ", ð‘1\n",
        "â€²\n",
        ", ð‘Š2\n",
        "â€²\n",
        ", ð‘2\n",
        "â€²\n",
        ", ð‘Š3\n",
        "â€²\n",
        ", ð‘3\n",
        "â€²\n"
      ],
      "metadata": {
        "id": "E2f5lcnL5NpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# x: [60k, 28, 28],\n",
        "# y: [60k]\n",
        "(x, y), _ = datasets.mnist.load_data()\n",
        "# x: [0~255] => [0~1.]\n",
        "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
        "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
        "\n",
        "print(x.shape, y.shape, x.dtype, y.dtype)\n",
        "print(tf.reduce_min(x), tf.reduce_max(x))\n",
        "print(tf.reduce_min(y), tf.reduce_max(y))\n",
        "\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
        "train_iter = iter(train_db)\n",
        "sample = next(train_iter)\n",
        "print('batch:', sample[0].shape, sample[1].shape)\n",
        "\n",
        "# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
        "# [dim_in, dim_out], [dim_out]\n",
        "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
        "b1 = tf.Variable(tf.zeros([256]))\n",
        "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
        "b2 = tf.Variable(tf.zeros([128]))\n",
        "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
        "b3 = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "for epoch in range(10):  # iterate db for 10\n",
        "    for step, (x, y) in enumerate(train_db):  # for every batch\n",
        "        # x:[128, 28, 28]\n",
        "        # y: [128]\n",
        "\n",
        "        # [b, 28, 28] => [b, 28*28]\n",
        "        x = tf.reshape(x, [-1, 28 * 28])\n",
        "\n",
        "        with tf.GradientTape() as tape:  # tf.Variable\n",
        "            # x: [b, 28*28]\n",
        "            # h1 = x@w1 + b1\n",
        "            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
        "            h1 = x @ w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
        "            h1 = tf.nn.relu(h1)\n",
        "            # [b, 256] => [b, 128]\n",
        "            h2 = h1 @ w2 + b2\n",
        "            h2 = tf.nn.relu(h2)\n",
        "            # [b, 128] => [b, 10]\n",
        "            out = h2 @ w3 + b3\n",
        "\n",
        "            # compute loss\n",
        "            # out: [b, 10]\n",
        "            # y: [b] => [b, 10]\n",
        "            y_onehot = tf.one_hot(y, depth=10)\n",
        "\n",
        "            # mse = mean(sum(y-out)^2)\n",
        "            # [b, 10]\n",
        "            loss = tf.square(y_onehot - out)\n",
        "            # mean: scalar\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # compute gradients\n",
        "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
        "        # print(grads)\n",
        "        # w1 = w1 - lr * w1_grad\n",
        "        w1.assign_sub(lr * grads[0])\n",
        "        b1.assign_sub(lr * grads[1])\n",
        "        w2.assign_sub(lr * grads[2])\n",
        "        b2.assign_sub(lr * grads[3])\n",
        "        w3.assign_sub(lr * grads[4])\n",
        "        b3.assign_sub(lr * grads[5])\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(epoch, step, 'loss:', float(loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFVVO-L86vBh",
        "outputId": "f6ef6972-77ca-425e-998a-6ae12eae7159"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
            "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n",
            "batch: (128, 28, 28) (128,)\n",
            "0 0 loss: 0.5511699914932251\n",
            "0 100 loss: 0.21255192160606384\n",
            "0 200 loss: 0.1738269180059433\n",
            "0 300 loss: 0.15766045451164246\n",
            "0 400 loss: 0.16910482943058014\n",
            "1 0 loss: 0.14802569150924683\n",
            "1 100 loss: 0.14464299380779266\n",
            "1 200 loss: 0.13876572251319885\n",
            "1 300 loss: 0.13339461386203766\n",
            "1 400 loss: 0.14206723868846893\n",
            "2 0 loss: 0.12693972885608673\n",
            "2 100 loss: 0.1271876096725464\n",
            "2 200 loss: 0.12184516340494156\n",
            "2 300 loss: 0.11903808265924454\n",
            "2 400 loss: 0.12596118450164795\n",
            "3 0 loss: 0.11372270435094833\n",
            "3 100 loss: 0.11545100063085556\n",
            "3 200 loss: 0.11040804535150528\n",
            "3 300 loss: 0.10914850234985352\n",
            "3 400 loss: 0.11500175297260284\n",
            "4 0 loss: 0.10449453443288803\n",
            "4 100 loss: 0.10693683475255966\n",
            "4 200 loss: 0.10205531120300293\n",
            "4 300 loss: 0.10178650915622711\n",
            "4 400 loss: 0.10688644647598267\n",
            "5 0 loss: 0.09760003536939621\n",
            "5 100 loss: 0.1003904938697815\n",
            "5 200 loss: 0.09565962851047516\n",
            "5 300 loss: 0.09611059725284576\n",
            "5 400 loss: 0.10070955753326416\n",
            "6 0 loss: 0.09225090593099594\n",
            "6 100 loss: 0.09524568915367126\n",
            "6 200 loss: 0.09060056507587433\n",
            "6 300 loss: 0.09157191216945648\n",
            "6 400 loss: 0.09585137665271759\n",
            "7 0 loss: 0.08790570497512817\n",
            "7 100 loss: 0.09102727472782135\n",
            "7 200 loss: 0.0865168645977974\n",
            "7 300 loss: 0.08779328316450119\n",
            "7 400 loss: 0.09187257289886475\n",
            "8 0 loss: 0.08429468423128128\n",
            "8 100 loss: 0.08746732771396637\n",
            "8 200 loss: 0.08308492600917816\n",
            "8 300 loss: 0.08458181470632553\n",
            "8 400 loss: 0.08855189383029938\n",
            "9 0 loss: 0.08120150864124298\n",
            "9 100 loss: 0.08443319797515869\n",
            "9 200 loss: 0.08018611371517181\n",
            "9 300 loss: 0.08180578798055649\n",
            "9 400 loss: 0.0857626274228096\n"
          ]
        }
      ]
    }
  ]
}