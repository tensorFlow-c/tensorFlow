{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensor09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQZpbSovvmCQ9oGCZQ20QY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhoujiuzhou9/tensorFlow/blob/V1/tensor09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0D1c9MdppvY",
        "outputId": "d152b1a5-ab9a-44c0-d33f-3fc950e183eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "print(tf.__version__)\n",
        "\n",
        "x = tf.constant([[2, 2], [3, 3]])\n",
        "y = tf.constant([[8, 16], [2, 3]])\n",
        "print(x @ y)\n",
        "\n",
        "a = tf.ones([4, 2, 3])\n",
        "b = tf.ones([4, 3, 5])\n",
        "c = a @ b\n",
        "print(c.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsz_jDce2B3i",
        "outputId": "f42b53af-4cd6-4623-90cc-0a0fe81cd408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n",
            "tf.Tensor(\n",
            "[[20 38]\n",
            " [30 57]], shape=(2, 2), dtype=int32)\n",
            "(4, 2, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recap\n",
        "â–ª ð‘œð‘¢ð‘¡ = ð‘Ÿð‘’ð‘™ð‘¢{ð‘Ÿð‘’ð‘™ð‘¢ ð‘Ÿð‘’ð‘™ð‘¢ ð‘‹@ð‘Š1 + ð‘1 @ð‘Š2 + ð‘2 @ð‘Š3 + ð‘3}\n",
        "\n",
        "â–ª ð‘ð‘Ÿð‘’ð‘‘ = ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð‘œð‘¢ð‘¡)\n",
        "\n",
        "â–ª ð‘™ð‘œð‘ ð‘  = ð‘€ð‘†ð¸(ð‘œð‘¢ð‘¡, ð‘™ð‘Žð‘ð‘’ð‘™)\n",
        "\n",
        "â–ª minimize ð‘™ð‘œð‘ ð‘ \n",
        "\n",
        "â–ª [ð‘Š1\n",
        "\n",
        "â€²\n",
        ", ð‘1\n",
        "â€²\n",
        ", ð‘Š2\n",
        "â€²\n",
        ", ð‘2\n",
        "â€²\n",
        ", ð‘Š3\n",
        "â€²\n",
        ", ð‘3\n",
        "â€²\n"
      ],
      "metadata": {
        "id": "E2f5lcnL5NpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# x: [60k, 28, 28],\n",
        "# y: [60k]\n",
        "(x, y), _ = datasets.mnist.load_data()\n",
        "# x: [0~255] => [0~1.]\n",
        "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
        "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
        "\n",
        "print(x.shape, y.shape, x.dtype, y.dtype)\n",
        "print(tf.reduce_min(x), tf.reduce_max(x))\n",
        "print(tf.reduce_min(y), tf.reduce_max(y))\n",
        "\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
        "train_iter = iter(train_db)\n",
        "sample = next(train_iter)\n",
        "print('batch:', sample[0].shape, sample[1].shape)\n",
        "\n",
        "# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
        "# [dim_in, dim_out], [dim_out]\n",
        "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
        "b1 = tf.Variable(tf.zeros([256]))\n",
        "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
        "b2 = tf.Variable(tf.zeros([128]))\n",
        "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
        "b3 = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "for epoch in range(10):  # iterate db for 10\n",
        "    for step, (x, y) in enumerate(train_db):  # for every batch\n",
        "        # x:[128, 28, 28]\n",
        "        # y: [128]\n",
        "\n",
        "        # [b, 28, 28] => [b, 28*28]\n",
        "        x = tf.reshape(x, [-1, 28 * 28])\n",
        "\n",
        "        with tf.GradientTape() as tape:  # tf.Variable\n",
        "            # x: [b, 28*28]\n",
        "            # h1 = x@w1 + b1\n",
        "            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
        "            h1 = x @ w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
        "            h1 = tf.nn.relu(h1)\n",
        "            # [b, 256] => [b, 128]\n",
        "            h2 = h1 @ w2 + b2\n",
        "            h2 = tf.nn.relu(h2)\n",
        "            # [b, 128] => [b, 10]\n",
        "            out = h2 @ w3 + b3\n",
        "\n",
        "            # compute loss\n",
        "            # out: [b, 10]\n",
        "            # y: [b] => [b, 10]\n",
        "            y_onehot = tf.one_hot(y, depth=10)\n",
        "\n",
        "            # mse = mean(sum(y-out)^2)\n",
        "            # [b, 10]\n",
        "            loss = tf.square(y_onehot - out)\n",
        "            # mean: scalar\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # compute gradients\n",
        "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
        "        # print(grads)\n",
        "        # w1 = w1 - lr * w1_grad\n",
        "        w1.assign_sub(lr * grads[0])\n",
        "        b1.assign_sub(lr * grads[1])\n",
        "        w2.assign_sub(lr * grads[2])\n",
        "        b2.assign_sub(lr * grads[3])\n",
        "        w3.assign_sub(lr * grads[4])\n",
        "        b3.assign_sub(lr * grads[5])\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(epoch, step, 'loss:', float(loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFVVO-L86vBh",
        "outputId": "f6ef6972-77ca-425e-998a-6ae12eae7159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
            "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n",
            "batch: (128, 28, 28) (128,)\n",
            "0 0 loss: 0.5511699914932251\n",
            "0 100 loss: 0.21255192160606384\n",
            "0 200 loss: 0.1738269180059433\n",
            "0 300 loss: 0.15766045451164246\n",
            "0 400 loss: 0.16910482943058014\n",
            "1 0 loss: 0.14802569150924683\n",
            "1 100 loss: 0.14464299380779266\n",
            "1 200 loss: 0.13876572251319885\n",
            "1 300 loss: 0.13339461386203766\n",
            "1 400 loss: 0.14206723868846893\n",
            "2 0 loss: 0.12693972885608673\n",
            "2 100 loss: 0.1271876096725464\n",
            "2 200 loss: 0.12184516340494156\n",
            "2 300 loss: 0.11903808265924454\n",
            "2 400 loss: 0.12596118450164795\n",
            "3 0 loss: 0.11372270435094833\n",
            "3 100 loss: 0.11545100063085556\n",
            "3 200 loss: 0.11040804535150528\n",
            "3 300 loss: 0.10914850234985352\n",
            "3 400 loss: 0.11500175297260284\n",
            "4 0 loss: 0.10449453443288803\n",
            "4 100 loss: 0.10693683475255966\n",
            "4 200 loss: 0.10205531120300293\n",
            "4 300 loss: 0.10178650915622711\n",
            "4 400 loss: 0.10688644647598267\n",
            "5 0 loss: 0.09760003536939621\n",
            "5 100 loss: 0.1003904938697815\n",
            "5 200 loss: 0.09565962851047516\n",
            "5 300 loss: 0.09611059725284576\n",
            "5 400 loss: 0.10070955753326416\n",
            "6 0 loss: 0.09225090593099594\n",
            "6 100 loss: 0.09524568915367126\n",
            "6 200 loss: 0.09060056507587433\n",
            "6 300 loss: 0.09157191216945648\n",
            "6 400 loss: 0.09585137665271759\n",
            "7 0 loss: 0.08790570497512817\n",
            "7 100 loss: 0.09102727472782135\n",
            "7 200 loss: 0.0865168645977974\n",
            "7 300 loss: 0.08779328316450119\n",
            "7 400 loss: 0.09187257289886475\n",
            "8 0 loss: 0.08429468423128128\n",
            "8 100 loss: 0.08746732771396637\n",
            "8 200 loss: 0.08308492600917816\n",
            "8 300 loss: 0.08458181470632553\n",
            "8 400 loss: 0.08855189383029938\n",
            "9 0 loss: 0.08120150864124298\n",
            "9 100 loss: 0.08443319797515869\n",
            "9 200 loss: 0.08018611371517181\n",
            "9 300 loss: 0.08180578798055649\n",
            "9 400 loss: 0.0857626274228096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "x = tf.constant([1, 4])\n",
        "y = tf.constant([2, 5])\n",
        "z = tf.constant([3, 6])\n",
        "tf.stack([x, y, z])\n",
        "\n",
        "tf.stack([x, y, z], axis=1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZTYWwWdHSrK",
        "outputId": "a5f7e046-b4df-4202-8e91-d574dd5f3d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "a=tf.ones([3,3])\n",
        "b=tf.norm(a)\n",
        "\n",
        "print(b.numpy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwRFkbPRKIDX",
        "outputId": "0455c04f-d5f5-410c-a3cb-c3365e051975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method _EagerTensorBase.numpy of <tf.Tensor: shape=(), dtype=float32, numpy=3.0>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "tf.random.set_seed(2467)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.shape[0]\n",
        "\n",
        "    pred = tf.math.top_k(output, maxk).indices\n",
        "    pred = tf.transpose(pred, perm=[1, 0])\n",
        "    target_ = tf.broadcast_to(target, pred.shape)\n",
        "    # [10, b]\n",
        "    correct = tf.equal(pred, target_)\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = tf.cast(tf.reshape(correct[:k], [-1]), dtype=tf.float32)\n",
        "        correct_k = tf.reduce_sum(correct_k)\n",
        "        acc = float(correct_k * (100.0 / batch_size))\n",
        "        res.append(acc)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "output = tf.random.normal([10, 6])\n",
        "output = tf.math.softmax(output, axis=1)\n",
        "target = tf.random.uniform([10], maxval=6, dtype=tf.int32)\n",
        "print('prob:', output.numpy())\n",
        "pred = tf.argmax(output, axis=1)\n",
        "print('pred:', pred.numpy())\n",
        "print('label:', target.numpy())\n",
        "\n",
        "acc = accuracy(output, target, topk=(1, 2, 3, 4, 5, 6))\n",
        "print('top-1-6 acc:', acc)\n"
      ],
      "metadata": {
        "id": "AfIUDJSLKIIi",
        "outputId": "512a6048-98f1-4bc4-ba1e-3f692e16b061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prob: [[0.25310278 0.21715644 0.16043882 0.13088997 0.04334084 0.19507109]\n",
            " [0.05892418 0.04548917 0.00926314 0.14529602 0.66777605 0.07325139]\n",
            " [0.09742809 0.08304427 0.07460099 0.04067177 0.626185   0.07806987]\n",
            " [0.20478569 0.12294925 0.12010485 0.13751233 0.36418733 0.05046057]\n",
            " [0.11872064 0.31072393 0.12530337 0.15528883 0.21325873 0.07670453]\n",
            " [0.01519807 0.09672114 0.1460476  0.00934331 0.5649092  0.1677807 ]\n",
            " [0.04199061 0.18141054 0.06647632 0.6006175  0.03198383 0.07752118]\n",
            " [0.09226219 0.23460893 0.13022321 0.16295876 0.05362028 0.32632664]\n",
            " [0.07019574 0.08611772 0.10912607 0.10521299 0.2152082  0.4141393 ]\n",
            " [0.01882887 0.2659769  0.19122466 0.2410926  0.14920163 0.1336753 ]]\n",
            "pred: [0 4 4 4 1 4 3 5 5 1]\n",
            "label: [0 2 3 4 2 4 2 3 5 5]\n",
            "top-1-6 acc: [40.0, 40.0, 50.0, 70.0, 80.0, 100.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, optimizers\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "print(tf.__version__)\n",
        "\n",
        "(x, y), _ = datasets.mnist.load_data()\n",
        "x = tf.convert_to_tensor(x, dtype=tf.float32) / 50.\n",
        "y = tf.convert_to_tensor(y)\n",
        "y = tf.one_hot(y, depth=10)\n",
        "print('x:', x.shape, 'y:', y.shape)\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128).repeat(30)\n",
        "x, y = next(iter(train_db))\n",
        "print('sample:', x.shape, y.shape)\n",
        "\n",
        "\n",
        "# print(x[0], y[0])\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 784 => 512\n",
        "    w1, b1 = tf.Variable(tf.random.truncated_normal([784, 512], stddev=0.1)), tf.Variable(tf.zeros([512]))\n",
        "    # 512 => 256\n",
        "    w2, b2 = tf.Variable(tf.random.truncated_normal([512, 256], stddev=0.1)), tf.Variable(tf.zeros([256]))\n",
        "    # 256 => 10\n",
        "    w3, b3 = tf.Variable(tf.random.truncated_normal([256, 10], stddev=0.1)), tf.Variable(tf.zeros([10]))\n",
        "\n",
        "    optimizer = optimizers.SGD(lr=0.01)\n",
        "\n",
        "    for step, (x, y) in enumerate(train_db):\n",
        "\n",
        "        # [b, 28, 28] => [b, 784]\n",
        "        x = tf.reshape(x, (-1, 784))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # layer1.\n",
        "            h1 = x @ w1 + b1\n",
        "            h1 = tf.nn.relu(h1)\n",
        "            # layer2\n",
        "            h2 = h1 @ w2 + b2\n",
        "            h2 = tf.nn.relu(h2)\n",
        "            # output\n",
        "            out = h2 @ w3 + b3\n",
        "            # out = tf.nn.relu(out)\n",
        "\n",
        "            # compute loss\n",
        "            # [b, 10] - [b, 10]\n",
        "            loss = tf.square(y - out)\n",
        "            # [b, 10] => [b]\n",
        "            loss = tf.reduce_mean(loss, axis=1)\n",
        "            # [b] => scalar\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # compute gradient\n",
        "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
        "        # print('==before==')\n",
        "        # for g in grads:\n",
        "        #     print(tf.norm(g))\n",
        "\n",
        "        grads, _ = tf.clip_by_global_norm(grads, 15)\n",
        "\n",
        "        # print('==after==')\n",
        "        # for g in grads:\n",
        "        #     print(tf.norm(g))\n",
        "        # update w' = w - lr*grad\n",
        "        optimizer.apply_gradients(zip(grads, [w1, b1, w2, b2, w3, b3]))\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(step, 'loss:', float(loss))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8uEwbAXQqoX",
        "outputId": "44503585-ccb7-4632-df01-3db5aaecc49b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "x: (60000, 28, 28) y: (60000, 10)\n",
            "sample: (128, 28, 28) (128, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss: 17.707599639892578\n",
            "100 loss: 0.5134084820747375\n",
            "200 loss: 0.16467225551605225\n",
            "300 loss: 0.11890697479248047\n",
            "400 loss: 0.09359651803970337\n",
            "500 loss: 0.08339296281337738\n",
            "600 loss: 0.0911116674542427\n",
            "700 loss: 0.0723511353135109\n",
            "800 loss: 0.07976216077804565\n",
            "900 loss: 0.06177585572004318\n",
            "1000 loss: 0.06797155737876892\n",
            "1100 loss: 0.06429316848516464\n",
            "1200 loss: 0.06276744604110718\n",
            "1300 loss: 0.06567469984292984\n",
            "1400 loss: 0.045643482357263565\n",
            "1500 loss: 0.053237657994031906\n",
            "1600 loss: 0.060986828058958054\n",
            "1700 loss: 0.06651026755571365\n",
            "1800 loss: 0.06772265583276749\n",
            "1900 loss: 0.049538783729076385\n",
            "2000 loss: 0.052303433418273926\n",
            "2100 loss: 0.045730143785476685\n",
            "2200 loss: 0.05582738667726517\n",
            "2300 loss: 0.05114409327507019\n",
            "2400 loss: 0.046876005828380585\n",
            "2500 loss: 0.049662940204143524\n",
            "2600 loss: 0.05392695218324661\n",
            "2700 loss: 0.05266182869672775\n",
            "2800 loss: 0.04031388834118843\n",
            "2900 loss: 0.0391383059322834\n",
            "3000 loss: 0.04520463943481445\n",
            "3100 loss: 0.035489216446876526\n",
            "3200 loss: 0.05573313310742378\n",
            "3300 loss: 0.04036179929971695\n",
            "3400 loss: 0.03799588605761528\n",
            "3500 loss: 0.04465365409851074\n",
            "3600 loss: 0.03976833447813988\n",
            "3700 loss: 0.04348215088248253\n",
            "3800 loss: 0.03551231324672699\n",
            "3900 loss: 0.03931925445795059\n",
            "4000 loss: 0.042120739817619324\n",
            "4100 loss: 0.03114621341228485\n",
            "4200 loss: 0.03419739380478859\n",
            "4300 loss: 0.03686544671654701\n",
            "4400 loss: 0.030260350555181503\n",
            "4500 loss: 0.03334924578666687\n",
            "4600 loss: 0.03262479230761528\n",
            "4700 loss: 0.036706022918224335\n",
            "4800 loss: 0.040548354387283325\n",
            "4900 loss: 0.03433087095618248\n",
            "5000 loss: 0.038993269205093384\n",
            "5100 loss: 0.022388286888599396\n",
            "5200 loss: 0.030766915529966354\n",
            "5300 loss: 0.02664731815457344\n",
            "5400 loss: 0.03432944416999817\n",
            "5500 loss: 0.02916622906923294\n",
            "5600 loss: 0.03723051771521568\n",
            "5700 loss: 0.028933018445968628\n",
            "5800 loss: 0.042082179337739944\n",
            "5900 loss: 0.03541267290711403\n",
            "6000 loss: 0.03411729633808136\n",
            "6100 loss: 0.0285496786236763\n",
            "6200 loss: 0.029821641743183136\n",
            "6300 loss: 0.026659978553652763\n",
            "6400 loss: 0.02820943109691143\n",
            "6500 loss: 0.025243502110242844\n",
            "6600 loss: 0.028414979577064514\n",
            "6700 loss: 0.03480411320924759\n",
            "6800 loss: 0.03382439538836479\n",
            "6900 loss: 0.03024403005838394\n",
            "7000 loss: 0.023992249742150307\n",
            "7100 loss: 0.030389977619051933\n",
            "7200 loss: 0.023358575999736786\n",
            "7300 loss: 0.02276628650724888\n",
            "7400 loss: 0.02551252208650112\n",
            "7500 loss: 0.01717943139374256\n",
            "7600 loss: 0.03088301233947277\n",
            "7700 loss: 0.027887048199772835\n",
            "7800 loss: 0.02655831351876259\n",
            "7900 loss: 0.031132224947214127\n",
            "8000 loss: 0.027417520061135292\n",
            "8100 loss: 0.027090223506093025\n",
            "8200 loss: 0.03268849104642868\n",
            "8300 loss: 0.02332128956913948\n",
            "8400 loss: 0.020101051777601242\n",
            "8500 loss: 0.02989983931183815\n",
            "8600 loss: 0.027045002207159996\n",
            "8700 loss: 0.02949776127934456\n",
            "8800 loss: 0.027788184583187103\n",
            "8900 loss: 0.01706778258085251\n",
            "9000 loss: 0.023384207859635353\n",
            "9100 loss: 0.03276357799768448\n",
            "9200 loss: 0.02826365828514099\n",
            "9300 loss: 0.0341617614030838\n",
            "9400 loss: 0.020470278337597847\n",
            "9500 loss: 0.02214732952415943\n",
            "9600 loss: 0.02592768892645836\n",
            "9700 loss: 0.02830732800066471\n",
            "9800 loss: 0.023923028260469437\n",
            "9900 loss: 0.02082066237926483\n",
            "10000 loss: 0.02502383664250374\n",
            "10100 loss: 0.03119858354330063\n",
            "10200 loss: 0.02770731784403324\n",
            "10300 loss: 0.025567922741174698\n",
            "10400 loss: 0.0184866301715374\n",
            "10500 loss: 0.022649485617876053\n",
            "10600 loss: 0.02765272930264473\n",
            "10700 loss: 0.03506266698241234\n",
            "10800 loss: 0.01851435750722885\n",
            "10900 loss: 0.029231449589133263\n",
            "11000 loss: 0.02644750475883484\n",
            "11100 loss: 0.02231307327747345\n",
            "11200 loss: 0.036674320697784424\n",
            "11300 loss: 0.02798885852098465\n",
            "11400 loss: 0.023576201871037483\n",
            "11500 loss: 0.029519598931074142\n",
            "11600 loss: 0.02553790621459484\n",
            "11700 loss: 0.0195132028311491\n",
            "11800 loss: 0.0199308879673481\n",
            "11900 loss: 0.031281977891922\n",
            "12000 loss: 0.023846637457609177\n",
            "12100 loss: 0.01918008178472519\n",
            "12200 loss: 0.027211163192987442\n",
            "12300 loss: 0.02384323626756668\n",
            "12400 loss: 0.02759910374879837\n",
            "12500 loss: 0.02055249735713005\n",
            "12600 loss: 0.02574082277715206\n",
            "12700 loss: 0.02815268188714981\n",
            "12800 loss: 0.03183613717556\n",
            "12900 loss: 0.023369085043668747\n",
            "13000 loss: 0.022495660930871964\n",
            "13100 loss: 0.013240862637758255\n",
            "13200 loss: 0.027786223217844963\n",
            "13300 loss: 0.022116169333457947\n",
            "13400 loss: 0.023169202730059624\n",
            "13500 loss: 0.02188282646238804\n",
            "13600 loss: 0.023620396852493286\n",
            "13700 loss: 0.0258540790528059\n",
            "13800 loss: 0.019930489361286163\n",
            "13900 loss: 0.0277382992208004\n",
            "14000 loss: 0.0233994759619236\n"
          ]
        }
      ]
    }
  ]
}